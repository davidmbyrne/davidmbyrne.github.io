[
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "from perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nWe are looking at the perceptron algorithm in this post, understanding how it operates and converges, and observing some use cases. The source code for the implementation can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/perceptron-blog-post/perceptron.py\n\n\nThe perceptron algorithm I wrote has two main methods. First, I wrote a “fit” function which prepares our raw data for use in computation moving forward. The fit function first (say that 5 times fast) initializes an empty history vector, which we use to store the accuracy score of our algorithm after each iteration. We also initialize our weight vector w, and append a column of ones to our feature matrix X, which makes our computation of dot product far easier. The second major method is the perceptron update function, which is responsible for the incremental adjustments to our weight vector. This function operates as follows:  1) Choose a random point in X  2) Compute the dot product of the given point and the current weight vector w  3) If the dot product is less than 0 update w as follows:  - \\(w^{t+1} = w^t + (2y_i-1) * X_i\\)\n\n\n\n\n\nOur first experiment is a simple 3 dimensional case that is linear seperable.\n\nnp.random.seed(11)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can run our algorithm until convergence, print the resulting score, and plot a line using the resulting weight vector. The process is as follows: 1) Initialize our Perceptron object and fit the model 2) For the specified number of iterations\na. Run the predict function, which creates a vector of guesses \\(\\hat{y}\\)\nb. Score the current iteration and store the score value in the history vector\nc. Check if the score is equal to 1 and terminate the loop, otherwise perform the perceptron update step\n\np = Perceptron() \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 1.0\n\n\n\n#Line drawing function using weight vector w\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can also plot the progession of the score of our algorithm for each iteration. In this case we reach a score value of 1.0 before the max iterations\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn the case of data which is not linearly seperable, the perceptron algorithm does not converge. Convergence occurs in this case when we reach a perfect score, which is impossible here. Data such as this looks as follows:\n\nnp.random.seed(15)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe again can run our algorithm and plot our final weight vector as well as the score for each iteration.\n\np = Perceptron()   \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 0.97\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this case, since convergence is impossible, we reach the max number of iterations before converging to a score of 1.0.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn this case, we increase the number of features from 3 to 5\n\nnp.random.seed(21)\n\nn = 100\np_features = 5\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -1.7, -1.7), (1.7, 1.7, 1.7, 1.7)])\n\n\np = Perceptron()   \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 1.0\n\n\nDespite increasing the number of features, the perceptron algorithm can still converge. The dot product is simply calculated using more features, and the same update rule can be applied.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\nFor each iteration of our perceptron update we have to compute a dot product of two vectors, each with length p equal to the amount of features. In computing this dot product, we have to make p multiplications and p-1 additions. If we assume each of these operations to be time constant operations, the time compelxity of one step is 2p-1 or O(p)."
  },
  {
    "objectID": "posts/gradient-descent-blog-post/LRtest.html",
    "href": "posts/gradient-descent-blog-post/LRtest.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "class LogisticRegression:\n    \n    def __init__(self):\n        pass\n    \n    def score(self, X, y):\n        return np.sum((y == np.transpose(self.guess)))/(X.shape[0])\n    \n    def predict(self, X, w):\n        self.guess = (np.dot(X, w) >= 0).astype(int)\n        return self.guess\n        \n    def fit(self, X, y):\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    def sigmoid(self, z):\n        return 1 / (1+ np.exp(-z))\n    \n    def logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n    \n    def empirical_risk(self, X, y, loss, w):\n        y_hat = self.predict(X, w)\n        return loss(y_hat, y).mean()\n    \n    def gradient(self, w, X, y):\n        w = w.reshape(3,1)\n        sigdot = (np.dot(X, w) >= 0).astype(int)\n        return np.sum(np.multiply(X,(self.sigmoid(sigdot) - y.reshape(len(y),1))))\n    \n    def fit_stochastic(self, X, y):\n        prev_loss = np.inf\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        n = X.shape[0]\n        for j in np.arange(m_epochs):\n            \n            order = np.arange(n)\n            np.random.shuffle(order)\n\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = self.X_[batch,:]\n                y_batch = y[batch]\n                grad = self.gradient(self.w_, x_batch, y_batch) \n                self.w_ -= alpha*grad                      \n            new_loss = self.empirical_risk(self.X_, y, self.logistic_loss, self.w_)\n            self.loss_history.append(new_loss)\n            self.currentscore = self.score(LR.X_, y)\n            self.score_history.append(self.score)\n    \n            if np.isclose(new_loss, prev_loss):          \n                break\n            else:\n                prev_loss = new_loss\n        \n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()  \nalpha = 0.001\nprev_loss = np.inf\nLR.fit(X, y)\nfor i in range(1000):\n    LR.w_ -= alpha*LR.gradient(LR.w_, LR.X_, y)                      \n    new_loss = LR.empirical_risk(LR.X_, y, LR.logistic_loss, LR.w_)\n    LR.loss_history.append(new_loss)\n    score = LR.score(LR.X_, y)\n    LR.score_history.append(score)\n    \n    if np.isclose(new_loss, prev_loss):          \n        break\n    else:\n        prev_loss = new_loss\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 0.92\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nn = X.shape[0]\nfor j in np.arange(m_epochs):\n            \n    order = np.arange(n)\n    np.random.shuffle(order)\n\n    for batch in np.array_split(order, n // batch_size + 1):\n        x_batch = X[batch,:]\n        y_batch = y[batch]\n        grad = gradient(w, x_batch, y_batch) \n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.001\nbatch_size = 5`\nLR.fit_stochastic(X, y)\nprint(\"Final Score: \" + str(LR.currentscore))\n\nFinal Score: 0.93\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is David’s fun little blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A dive into implementation and usage of both normal and stochastic gradient descent\n\n\n\n\n\n\nMar 17, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA look at a basic machine learning model, The Perceptron Algorithm\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA look at a basic machine learning model, The Perceptron Algorithm\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA look at a basic machine learning model, The Perceptron Algorithm\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gradient-descent-blog-post/index.html",
    "href": "posts/gradient-descent-blog-post/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "from logisticregression import LogisticRegression\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons\nimport random\n\n\n\n\nSource code for this post can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/gradient-descent-blog-post/logisticregression.py\nThe implementation of gradient descent was rather straightforward. The gradient method was rather significant, and employed the logistic sigmoid function to calculate:\n\\[\\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(\\langle w,x_i \\rangle) - y_i)x_i\\] I used this value in the update step of the fit method, \\[w^{(i+1)} = w^{(i)} - \\alpha \\nabla L(w)\\] where alpha is the model’s learning rate. This update is called anytime that the current loss calculated by the empirical risk method is not computationally close to the previous loss. Once our loss function is successfully minimized, the algorithm converges to a value for \\(w\\) .\n\n\n\nOne of the variables that gradient decent utlizes is the learning rate, or alpha. We utilize this value in calculating a new weight vector \\(w\\), and it determines the impact that the gradient has in each step of modifying \\(w\\). A small value of alpha will make the steps from each iteration to the next smaller, while a larger alpha will cause larger jumps. When choosing an effective value for alpha, it is important to consider this balance. A very small alpha will cause the minimization of the loss function to take a long time to converge, but too large of an alpha value will make it impossible to converge at all. We can observe this case of non convergence.\n\nrandom.seed(14)\nX, y = make_moons(n_samples = 100, noise = 0.35)\n\n\n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.1\n\nLR.fit(X, y, m_epochs, alpha)\n\nConverged after 12 tries\n\n\n\n\n\n\nrandom.seed(14)\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 3\n\nLR.fit(X, y, m_epochs, alpha)\n\nReached maximum epochs before converging\n\n\n\n\n\n\nIn addition to our typical run of the mill gradient descent, we can implement stochastic gradient descent, which divides the full data set into “batches” or subsets of a given size and calculates the gradient over just that batch. We can use this to our advantage, as manipulating batch size can affect the speed at which the loss function converges.\n\np_features = 20\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 5\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 2.89\n\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 20\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 5.04\n\n\nIn this case, using a smaller batch size of 5 led to convergence in nearly half of the total epochs when compared to a batch size of 20. To see a greater trend, we can try all batch sizes from 2 to 30.\n\ntracker = []\nfor i in range(2,30):\n    term_arr = []\n    m_epochs = 1000\n    alpha = 0.05\n    batch_size = i+2\n    for i in range(100):\n        LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n        term_arr.append(LR.stop)\n    tracker.append(sum(term_arr)/100)\nfig = plt.plot(tracker)\nxlab = plt.xlabel(\"Batch Size\")\nylab = plt.ylabel(\"Average Epochs\")\n\n\n\n\nThe larger experiment confirms the previous belief, and smaller batch sizes have faster convergence times, with the change plateauing after a batch size of about 15."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport random\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\n\nData used in this blog post were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n\n\nThe aim of this post was to identify a subset of the available features that could be used to train a classification model. To accomplish this task, we must first prepare the data accordingly, which we do using the following function prepare_data. This function drops identifying information from the study which will not be useful in predictive modeling, as well as separating the target vector from the rest of the dataset.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_test, y_test = prepare_data(test)\n\nWith our data appropriately cleaned and partitioned, we can now begin to formulate a model. The model of choice here is a random forest, which is an ensemble method made up of multiple decision trees. These individual trees are created from random independent samples or subsets of data, whose decisions are averaged in making predictions. To effectively choose features that accurately predict penguin species, we can build multiple models using different, independent combinations of columns. We can evaluate each of these models and score them to quantify their performance and choose the combination which obtains the highest score. The following block of code selects a unique subset of columns, both qualitative and quantitative, trains a model using the partitioned training dataset, and tests its accuracy on the testing dataset.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import accuracy_score\n\nall_qual_cols = [\"Island\", \"Sex\", \"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    print(\"Columns: \" + str(cols))\n    rfc = RandomForestClassifier()\n    X_train_curr = X_train[cols]\n    rfc.fit(X_train_curr, y_train)\n    preds = rfc.predict(X_test[cols])\n    print(\"Model Score: \" + str(accuracy_score(y_test, preds)))\n    \n    \n\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 0.9852941176470589\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 0.9852941176470589\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 0.8676470588235294\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 0.9852941176470589\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 1.0\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 0.7941176470588235\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 0.9705882352941176\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 0.9705882352941176\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 0.8088235294117647\n\n\nWhile many of the models achieved high accuracy scores, only one achieved 100% testing accuracy. The features used in the creation of this model were Culmen Length, Flipper Length, and Sex. To get a better understanding of why or how these features led to the best model, we can look at some summary statistics at the species level.\n\ntrain.groupby([\"Species\", \"Sex\"])[[\"Culmen Length (mm)\", \"Flipper Length (mm)\"]].mean()\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      Species\n      Sex\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      37.100000\n      187.719298\n    \n    \n      MALE\n      40.458182\n      192.690909\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      46.424138\n      191.551724\n    \n    \n      MALE\n      51.185185\n      199.666667\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      44.500000\n      217.000000\n    \n    \n      FEMALE\n      45.600000\n      212.928571\n    \n    \n      MALE\n      49.592593\n      221.462963\n    \n  \n\n\n\n\nLooking at the average culmen and flipper lengths across species and sexes, we notice that Gentoo penguins have a notably high flipper length and Adelie penguins have shorter culmens compared to the other species. To better understand this distinction we can visualize these metrics using a scatter plot, faceted by sex.\n\ng = sns.FacetGrid(data = train, col = 'Sex')\ng.map(sns.scatterplot, \"Culmen Length (mm)\", \"Flipper Length (mm)\", 'Species')\n\n<seaborn.axisgrid.FacetGrid at 0x7fe72169df40>\n\n\n\n\n\nInteresting point to note: one of the penguins did not have a sex recorded, and this penguin is plotted on its own graph where sex = “.” located at the far right. With the data plotted, it becomes easy to see why this choice of features led to such a high accuracy. The data initially appears to be seperable, with only a few instances of near overlap.\n\ncolors = {\"Adelie Penguin (Pygoscelis adeliae)\" : \"Red\", \"Chinstrap penguin (Pygoscelis antarctica)\" : \"Blue\", \"Gentoo penguin (Pygoscelis papua)\" : \"Green\"}\nfig = plt.scatter(train[\"Flipper Length (mm)\"], train[\"Culmen Depth (mm)\"], c=train['Species'].map(colors))\n\n\n\n\n\nfig = plt.scatter(train[\"Culmen Length (mm)\"], train[\"Culmen Depth (mm)\"], c=train['Species'].map(colors))\n\n\n\n\n\nfig = plt.scatter(train[\"Culmen Length (mm)\"], train[\"Flipper Length (mm)\"], c=train['Species'].map(colors))\n\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nrfc = RandomForestClassifier()\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Sex_FEMALE\", \"Sex_MALE\"]\nrfc.fit(X_train[cols], y_train)\nprint(rfc.score(X_train[cols], y_train))\n\nplot_regions(rfc, X_train[cols], y_train)\n\n1.0"
  },
  {
    "objectID": "posts/timnit-gebru/Untitled.html",
    "href": "posts/timnit-gebru/Untitled.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics.\n\n\nThe video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s. In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy.\nDr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\nDr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\nDr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well.\ntl;dr With great power comes great responsibility\n\n\n\nDo you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
  },
  {
    "objectID": "posts/timnit-gebru/index.html",
    "href": "posts/timnit-gebru/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics.\n\n\nThe video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s. In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy.\nDr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\nDr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\nDr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well.\ntl;dr With great power comes great responsibility\n\n\n\nDo you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
  }
]