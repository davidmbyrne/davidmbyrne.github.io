[
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "from perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nWe are looking at the perceptron algorithm in this post, understanding how it operates and converges, and observing some use cases. The source code for the implementation can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/perceptron-blog-post/perceptron.py\n\n\nThe perceptron algorithm I wrote has two main methods. First, I wrote a “fit” function which prepares our raw data for use in computation moving forward. The fit function first (say that 5 times fast) initializes an empty history vector, which we use to store the accuracy score of our algorithm after each iteration. We also initialize our weight vector w, and append a column of ones to our feature matrix X, which makes our computation of dot product far easier. The second major method is the perceptron update function, which is responsible for the incremental adjustments to our weight vector. This function operates as follows:  1) Choose a random point in X  2) Compute the dot product of the given point and the current weight vector w  3) If the dot product is less than 0 update w as follows:  - \\(w^{t+1} = w^t + (2y_i-1) * X_i\\)\n\n\n\n\n\nOur first experiment is a simple 3 dimensional case that is linear seperable.\n\nnp.random.seed(11)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can run our algorithm until convergence, print the resulting score, and plot a line using the resulting weight vector. The process is as follows: 1) Initialize our Perceptron object and fit the model 2) For the specified number of iterations\na. Run the predict function, which creates a vector of guesses \\(\\hat{y}\\)\nb. Score the current iteration and store the score value in the history vector\nc. Check if the score is equal to 1 and terminate the loop, otherwise perform the perceptron update step\n\np = Perceptron() \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 1.0\n\n\n\n#Line drawing function using weight vector w\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can also plot the progession of the score of our algorithm for each iteration. In this case we reach a score value of 1.0 before the max iterations\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn the case of data which is not linearly seperable, the perceptron algorithm does not converge. Convergence occurs in this case when we reach a perfect score, which is impossible here. Data such as this looks as follows:\n\nnp.random.seed(15)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe again can run our algorithm and plot our final weight vector as well as the score for each iteration.\n\np = Perceptron()   \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 0.97\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this case, since convergence is impossible, we reach the max number of iterations before converging to a score of 1.0.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn this case, we increase the number of features from 3 to 5\n\nnp.random.seed(21)\n\nn = 100\np_features = 5\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -1.7, -1.7), (1.7, 1.7, 1.7, 1.7)])\n\n\np = Perceptron()   \nmax_iter = 1000 #Set max iterations\np.fit(X, y, max_iter) #Fit model\n    \nprint(\"final score: \" + str(p.currscore))\n\nfinal score: 1.0\n\n\nDespite increasing the number of features, the perceptron algorithm can still converge. The dot product is simply calculated using more features, and the same update rule can be applied.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\nFor each iteration of our perceptron update we have to compute a dot product of two vectors, each with length p equal to the amount of features. In computing this dot product, we have to make p multiplications and p-1 additions. If we assume each of these operations to be time constant operations, the time compelxity of one step is 2p-1 or O(p)."
  },
  {
    "objectID": "posts/gradient-descent-blog-post/LRtest.html",
    "href": "posts/gradient-descent-blog-post/LRtest.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "class LogisticRegression:\n    \n    def __init__(self):\n        pass\n    \n    def score(self, X, y):\n        return np.sum((y == np.transpose(self.guess)))/(X.shape[0])\n    \n    def predict(self, X, w):\n        self.guess = (np.dot(X, w) >= 0).astype(int)\n        return self.guess\n        \n    def fit(self, X, y):\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    def sigmoid(self, z):\n        return 1 / (1+ np.exp(-z))\n    \n    def logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n    \n    def empirical_risk(self, X, y, loss, w):\n        y_hat = self.predict(X, w)\n        return loss(y_hat, y).mean()\n    \n    def gradient(self, w, X, y):\n        w = w.reshape(3,1)\n        sigdot = (np.dot(X, w) >= 0).astype(int)\n        return np.sum(np.multiply(X,(self.sigmoid(sigdot) - y.reshape(len(y),1))))\n    \n    def fit_stochastic(self, X, y):\n        prev_loss = np.inf\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        n = X.shape[0]\n        for j in np.arange(m_epochs):\n            \n            order = np.arange(n)\n            np.random.shuffle(order)\n\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = self.X_[batch,:]\n                y_batch = y[batch]\n                grad = self.gradient(self.w_, x_batch, y_batch) \n                self.w_ -= alpha*grad                      \n            new_loss = self.empirical_risk(self.X_, y, self.logistic_loss, self.w_)\n            self.loss_history.append(new_loss)\n            self.currentscore = self.score(LR.X_, y)\n            self.score_history.append(self.score)\n    \n            if np.isclose(new_loss, prev_loss):          \n                break\n            else:\n                prev_loss = new_loss\n        \n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()  \nalpha = 0.001\nprev_loss = np.inf\nLR.fit(X, y)\nfor i in range(1000):\n    LR.w_ -= alpha*LR.gradient(LR.w_, LR.X_, y)                      \n    new_loss = LR.empirical_risk(LR.X_, y, LR.logistic_loss, LR.w_)\n    LR.loss_history.append(new_loss)\n    score = LR.score(LR.X_, y)\n    LR.score_history.append(score)\n    \n    if np.isclose(new_loss, prev_loss):          \n        break\n    else:\n        prev_loss = new_loss\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 0.92\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nn = X.shape[0]\nfor j in np.arange(m_epochs):\n            \n    order = np.arange(n)\n    np.random.shuffle(order)\n\n    for batch in np.array_split(order, n // batch_size + 1):\n        x_batch = X[batch,:]\n        y_batch = y[batch]\n        grad = gradient(w, x_batch, y_batch) \n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.001\nbatch_size = 5`\nLR.fit_stochastic(X, y)\nprint(\"Final Score: \" + str(LR.currentscore))\n\nFinal Score: 0.93\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is David’s fun little blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451 Final Project: Create a machine learning model that predicts a players bat speed utilizing Recursive Feature Elimination, Linear Regression and Random Forest Regression models.\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler, David Byrne, Julia Fairbank, Sam Ehrsam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA dive into implementation and usage of both normal and stochastic gradient descent\n\n\n\n\n\n\nMar 17, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImage Compression and Spectral Community Detection\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDiscussing our meeting with Dr. Timnit Gebru\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA look at a basic machine learning model, The Perceptron Algorithm\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nBuilding a reproducible model for predicting penguin species\n\n\n\n\n\n\nMar 9, 2023\n\n\nDavid Byrne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gradient-descent-blog-post/index.html",
    "href": "posts/gradient-descent-blog-post/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "from logisticregression import LogisticRegression\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons\nimport random\n\n\n\n\nSource code for this post can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/gradient-descent-blog-post/logisticregression.py\nThe implementation of gradient descent was rather straightforward. The gradient method was rather significant, and employed the logistic sigmoid function to calculate:\n\\[\\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(\\langle w,x_i \\rangle) - y_i)x_i\\] I used this value in the update step of the fit method, \\[w^{(i+1)} = w^{(i)} - \\alpha \\nabla L(w)\\] where alpha is the model’s learning rate. This update is called anytime that the current loss calculated by the empirical risk method is not computationally close to the previous loss. Once our loss function is successfully minimized, the algorithm converges to a value for \\(w\\) .\n\n\n\nOne of the variables that gradient decent utlizes is the learning rate, or alpha. We utilize this value in calculating a new weight vector \\(w\\), and it determines the impact that the gradient has in each step of modifying \\(w\\). A small value of alpha will make the steps from each iteration to the next smaller, while a larger alpha will cause larger jumps. When choosing an effective value for alpha, it is important to consider this balance. A very small alpha will cause the minimization of the loss function to take a long time to converge, but too large of an alpha value will make it impossible to converge at all. We can observe this case of non convergence.\n\nrandom.seed(14)\nX, y = make_moons(n_samples = 100, noise = 0.35)\n\n\n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.1\n\nLR.fit(X, y, m_epochs, alpha)\n\nConverged after 12 tries\n\n\n\n\n\n\nrandom.seed(14)\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 3\n\nLR.fit(X, y, m_epochs, alpha)\n\nReached maximum epochs before converging\n\n\n\n\n\n\nIn addition to our typical run of the mill gradient descent, we can implement stochastic gradient descent, which divides the full data set into “batches” or subsets of a given size and calculates the gradient over just that batch. We can use this to our advantage, as manipulating batch size can affect the speed at which the loss function converges.\n\np_features = 20\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 5\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 2.89\n\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 20\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 5.04\n\n\nIn this case, using a smaller batch size of 5 led to convergence in nearly half of the total epochs when compared to a batch size of 20. To see a greater trend, we can try all batch sizes from 2 to 30.\n\ntracker = []\nfor i in range(2,30):\n    term_arr = []\n    m_epochs = 1000\n    alpha = 0.05\n    batch_size = i+2\n    for i in range(100):\n        LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n        term_arr.append(LR.stop)\n    tracker.append(sum(term_arr)/100)\nfig = plt.plot(tracker)\nxlab = plt.xlabel(\"Batch Size\")\nylab = plt.ylabel(\"Average Epochs\")\n\n\n\n\nThe larger experiment confirms the previous belief, and smaller batch sizes have faster convergence times, with the change plateauing after a batch size of about 15."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport random\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain = train.loc[train[\"Sex\"].isin(['MALE', 'FEMALE'])]\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\n\n\nData used in this blog post were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. The subjects of the data are these guys:\n\nThe Adelie Penguin,\n\nThe Chinstrap Penguin,\n\nand The Gentoo Penguin\n\n\n\n\nThe aim of this post was to identify a subset of the available features that could be used to train a classification model. To accomplish this task, we must first prepare the data accordingly, which we do using the following function prepare_data. This function drops identifying information from the study which will not be useful in predictive modeling, as well as separating the target vector from the rest of the dataset.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_test, y_test = prepare_data(test)\n\nWith our data appropriately cleaned and partitioned, we can now begin to formulate a model. The model of choice here is a random forest, which is an ensemble method made up of multiple decision trees. These individual trees are created from random independent samples or subsets of data, whose decisions are averaged in making predictions. To effectively choose features that accurately predict penguin species, we can build multiple models using different, independent combinations of columns. We can evaluate each of these models and score them to quantify their performance and choose the combination which obtains the highest score. The following block of code selects a unique subset of columns, both qualitative and quantitative, trains a model using the partitioned training dataset, and tests its accuracy.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import accuracy_score\n\nall_qual_cols = [\"Island\", \"Sex\", \"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    print(\"Columns: \" + str(cols))\n    rfc = RandomForestClassifier()\n    X_train_curr = X_train[cols]\n    rfc.fit(X_train_curr, y_train)\n    preds = rfc.predict(X_train[cols])\n    print(\"Model Score: \" + str(accuracy_score(y_train, preds)))\n    \n    \n\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 1.0\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 1.0\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nModel Score: 0.98046875\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 1.0\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 1.0\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\nModel Score: 0.9765625\nColumns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 1.0\nColumns: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 1.0\nColumns: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nModel Score: 0.98046875\n\n\nWe find that many of these combinations can create perfect accuracy models on testing data. In fact, all combinations besides those with culmen length and flipper length as the quantitative columns work just as well, regardless of qualitative factors. I would predict that utilizing sex along with physical measurements would likely yield high accuracy results for future data points, so I will begin by broadly choosing it as a qualitative feature as we continue to look for good quantitative factors. To get a better understanding of these factors and how they will contribute to a model we can look at some basic summary statistics.\n\ntrain.groupby([\"Species\", \"Sex\"])[[\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Culmen Depth (mm)\"]].mean()\n\n\n\n\n\n  \n    \n      \n      \n      Culmen Length (mm)\n      Flipper Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Species\n      Sex\n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      37.100000\n      187.719298\n      17.645614\n    \n    \n      MALE\n      40.458182\n      192.690909\n      19.116364\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      46.424138\n      191.551724\n      17.641379\n    \n    \n      MALE\n      51.185185\n      199.666667\n      19.303704\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      FEMALE\n      45.600000\n      212.928571\n      14.242857\n    \n    \n      MALE\n      49.592593\n      221.462963\n      15.687037\n    \n  \n\n\n\n\nFrom this table we can begin to identify pairs of features that have some level of seperability. Looking at the average culmen and flipper lengths across species and sexes, we notice that Gentoo penguins have a notably high flipper length and Adelie penguins have shorter culmens compared to the other species. To better understand this distinction we can visualize these metrics using a scatter plot, faceted by sex.\n\ng = sns.FacetGrid(data = train, col = 'Sex', hue = 'Species')\ng.map(sns.scatterplot, \"Culmen Length (mm)\", \"Flipper Length (mm)\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f8bbfe3fb20>\n\n\n\n\n\nWith the data plotted, it becomes easy to see why this choice of features led to such a high accuracy. The data initially appears to be seperable, with only a few instances of near overlap. We can clearly identify the Gentoo penguins now with their comparably long flipper length, denoted as the blue points of data. In a similar vein, the Adelie penguins are seen as orange points, with a similar flipper length to the Chinstrap penguins but a much shorter culmen. We also can note that thh distribtution of these measurements across sexes are similar, with the data holding the same shape across the faceted graphs.\n\n\n\nWith our features now appropriately selected, we can construct our final model and plot the associated decision regions. In order to plot our decision regions, we have constructed the plot_regions function below, which will create a similarly formatted set of graphs as we previously produced, but with shading showing the classifying regions and barriers. This will help us better understand how the model is making these decisions.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe then create our final Random Forest Classifying model, fitted using the training data, and plot the decision regions along with the test data.\n\nrfc = RandomForestClassifier()\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Sex_FEMALE\", \"Sex_MALE\"]\nrfc.fit(X_train[cols], y_train)\nprint(rfc.score(X_train[cols], y_train))\n\nplot_regions(rfc, X_test[cols], y_test)\n\n1.0\n\n\n\n\n\nWith the regions plotted, we can see exactly how the model makes predictions. We can also see potential overfitting in the female plot, where the Chinstrap region “leaks” into the Gentoo region. This was due to a notable point in the training data set, where a Chinstrap penguin had a notably short culmen. This did not prove to cause any inaccuracies in the scoring of our model, simply due to the fact that there were no data points near that region, but one could forsee that with additional data points, this could lead to inaccuracies. Generally speaking, however, the decision regions are rather well defined and in accordance with our exploratory analysis."
  },
  {
    "objectID": "posts/timnit-gebru/Untitled.html",
    "href": "posts/timnit-gebru/Untitled.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics.\n\n\nThe video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s. In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy.\nDr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\nDr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\nDr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well.\ntl;dr With great power comes great responsibility\n\n\n\nDo you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
  },
  {
    "objectID": "posts/timnit-gebru/index.html",
    "href": "posts/timnit-gebru/index.html",
    "title": "Dr. Timnit Gebru",
    "section": "",
    "text": "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics.\n\n\nThe video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s. In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy.\nDr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\nDr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\nDr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well.\ntl;dr With great power comes great responsibility\n\n\n\nDo you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
  },
  {
    "objectID": "posts/penguins-blog-post/Untitled.html",
    "href": "posts/penguins-blog-post/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import requests\nfrom bs4 import BeautifulSoup\n\n# Set the URL for Atwater dining hall\nurl = \"https://middlebury.nutrislice.com/menu/atwater-dining-hall/dinner/2023-05-03\"\n\n# Send a GET request to the URL and get the HTML content\nresponse = requests.get(url)\nhtml_content = response.content\n\n# Parse the HTML content using BeautifulSoup\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n# Find the section with the menu items\nmenu_section = soup.find('section', {'class': 'menu__section'})\nprint(menu_section)\n\n# Find all the menu items in the section\nmenu_items = menu_section.find_all('div', {'class': 'menu__item'})\n\n# Print the menu items for the past 7 days\nfor i in range(7):\n    print(f\"Menu for {menu_items[i].find('div', {'class': 'menu__item-name'}).text.strip()}:\")\n    menu_items_list = menu_items[i].find_all('div', {'class': 'menu__item-details'})\n    for item in menu_items_list:\n        print(f\"- {item.text.strip()}\")\n    print()\n\nNone\n\n\nAttributeError: 'NoneType' object has no attribute 'find_all'"
  },
  {
    "objectID": "posts/unsupervised-learning/index.html",
    "href": "posts/unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport PIL\nimport urllib\nfrom SVD import svd_reconstruct, read_image, to_greyscale, compare_images, svd_experiment"
  },
  {
    "objectID": "posts/unsupervised-learning/index.html#part-1-image-compression-with-the-singular-value-decomposition",
    "href": "posts/unsupervised-learning/index.html#part-1-image-compression-with-the-singular-value-decomposition",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Part 1: Image Compression with the Singular Value Decomposition",
    "text": "Part 1: Image Compression with the Singular Value Decomposition\nSingular Value Decomposition (SVD) is a factorization of some matrix A with the form\n\\[A  = UDV^T\\] where \\(U \\in \\mathbb{R}^{mxm}\\), \\(D \\in \\mathbb{R}^{mxn}\\), and \\(V \\in \\mathbb{R}^{nxn}\\). This specific factorization is so useful because we can approximate our initial matrix A by using a far smaller subset of information. In particular for some k, we can pull the first k columns of U, the first k rows of V, and the first k singular values of D to calculate our approximation of A. This is helpful in tasks such as image compression, where we may not have space to store such a large image and have to store a representative simplified image.\nThe goal of this post was to create a function which would take a SVD representation of an RGB image and reconstruct the image using an input value for k determining how many singular values to use.\nWe first read in our image from a given url and make the image a greyscale image.\n\nurl = \"https://www.nj.com/resizer/lEQnc80MkcYWitSy3-_PLb4yON0=/1280x0/smart/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/6VPZK7CRUJBFJM6G24CPF73ZAA.jpg\"\nimg = read_image(url)\ngrey_img = to_greyscale(img)\n\nWe then create a reconstruction of our image, specifiying a given k.\n\nreconstruction = svd_reconstruct(grey_img, 20)\n\nWe can then use the compare_images function to show the difference between our original image and the reconstructed, compressed image. The two images look side by side look like this:\n\ncompare_images(grey_img, reconstruction)\n\n\n\n\nYikes! Not very good! Let’s look at a few different values for k and see how this affects image quality. We would suspect that with a larger k value, and more values being stored, the compressed image will look more like the original image.\n\nsvd_experiment(grey_img)\n\n\n\n\nOur hypothesis would be true! As the number of components increases, the image quality and storage necessary also increase! The final image, with 60 components, requires a little more than 10% of the storage initially required and most of the image’s integrity remains."
  },
  {
    "objectID": "posts/unsupervised-learning/index.html#part-2-spectral-community-detection",
    "href": "posts/unsupervised-learning/index.html#part-2-spectral-community-detection",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Part 2: Spectral Community Detection",
    "text": "Part 2: Spectral Community Detection\nIn part 2 of this post, we are looking to implement spectral clustering on the Zachary’s Karate Club dataset, comparing it to how the club actually divided. To do so, I wrote my own implementation of spectral clustering below. The process is as follows:\n\nTake a graph object, get the adjacency matrix representation, and symmetrize it\nCreate the degree matrix D by summing the amount of edges each node has and placing the value on the diagonal.\nFind the normalized laplacian of the form: \\[L = D^{-1}(D-A)\\]\nUse the eigenvector of the second smallest eigenvalue to minimize the normcut objective\nUse the sign of the values of this eigenvector to create the communities\n\n\nimport networkx as nx\nimport numpy as np\n\ndef spectral_clustering(G):\n    \n    # Get the adjacency matrix from the graph\n    A = nx.adjacency_matrix(G).toarray()\n    \n    # Symmetrize the matrix\n    A = A + A.T\n    A[A > 1] = 1\n    \n    # Create our degree matrix D\n    D = np.diag(np.sum(A, axis = 0))\n    D1 = np.linalg.inv(D) # D inverse\n    \n    # Normalized laplacian\n    L = D1@(D-A) \n    \n    # Get eigenvalues and eigenvectors of L\n    w, v = np.linalg.eig(L)\n    \n    # Get eigenvector of the second smallest eigenvalue\n    z_ = v[:, 1]\n    \n    # Get the sign, use in clustering\n    z = z_ > 0\n    return z\n    \n\nWe can then use this implementation to identify the two potential communities and plot the nodes colored by these choices. I used the circular layout simply for ease of analysis.\n\nG = nx.karate_club_graph()\nz = spectral_clustering(G)\nlayout = nx.layout.circular_layout(G)\n\nnx.draw_networkx(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if i == 1 else \"steelblue\" for i in z],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n/var/folders/fl/5qx5y4sx03j07bk5cd1whd4c0000gn/T/ipykernel_9744/489024421.py:7: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G).toarray()\n\n\n\n\n\nPretty cool! There is a clear divide eith about half of the nodes being tied to node 0 (Mr. Hi) and the other half tied to node 33 (Officer). To see how well my implementation did, we can compare its predictions to the actual divide. The true divide is plotted below, again with the circular layout for ease of use.\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.circular_layout(G)\n\n\nclubs = nx.get_node_attributes(G, \"club\")\nnx.draw_networkx(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nThey’re almost the same! The only two points of contention are nodes 2 and 8. To better understand why these two points wre classified incorrectly, we can plot the nodes using the spring layout, which gives edges attractive forces and nodes repulsive forces, essentially positioning nodes with shared edges closer to each other.\n\nlayout = nx.layout.spring_layout(G)\n\n\nclubs = nx.get_node_attributes(G, \"club\")\nnx.draw_networkx(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nBy plotting the network in this manner, we can see that nodes 2 and 8 fall directly in the middle of the two main communities. This implies that these two individuals were almost equally likely to share edges and therefore be grouped into either community. Overall, our spectral community detection was successful and almost accurately identified every nodes related identity."
  },
  {
    "objectID": "posts/timnit-gebru/index.html#dr.-gebrus-talk-at-middlebury",
    "href": "posts/timnit-gebru/index.html#dr.-gebrus-talk-at-middlebury",
    "title": "Dr. Timnit Gebru",
    "section": "Dr. Gebru’s talk at Middlebury",
    "text": "Dr. Gebru’s talk at Middlebury"
  },
  {
    "objectID": "posts/final-project-post/index.html",
    "href": "posts/final-project-post/index.html",
    "title": "Predicting Bat Speed",
    "section": "",
    "text": "Abstract and Overview of Significance of Topic\nBat speed is a measure of player performance that has become increasingly popular for player development over the past decade. It is typically measured as the speed that the sweet spot of the bat (about 6 inches from the end of the barrel) is traveling when contact is made with the baseball. Bat speed has become increasingly popular due to its high correlation with exit velocity and subsequently hitting metrics such as expected batting average (xBA) or expected weighted on base average (xWOBA). Metrics such as xBA and xWOBA are modern metrics that are used to effectively quantify a players ability to contribute offensively in a positive manner. This increasing popularity in bat speed has led to a related increase in training methodologies based around developing it. Coaches across all levels of play use bat speed as a KPI to validate and inform individualized coaching decisions.\n\n\nFormal Introduction to Topic\nFor our final project, we are using data from the Open Biomechanics Project (OBP) driveline baseball research. Our data captures the biomechanical breakdown of a baseball player’s swing by measuring forces produced by different body parts in three dimensions over exact instances in time, for example, at instance X, player 103’s 2nd swing has a max lead hip force in the y direction of 111.73. The data was captured using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. The available data is rather robust, accounting for every piece of information that could be responsible for a baseball swing.\nFor our project, our goal is to create a machine learning model that uses this OBP data to identify the most important features of a player’s swing when generating bat speed, and then use those features to accurately predict a player’s bat speed. By comparing an athlete’s true bat speed to their predicted bat speed based on our model, the player could identify how efficiently they are swinging. We hope that this model could be used by baseball players and coaches to address the unique aspects of a player’s swing that could contribute to a higher bat speed, which in turn, would the players reach their fullest potential based on where their inefficiencies lie. Our project can be broken down into two main processes: Identifying the key features that contribute to bat speed. Creating a model that uses the key features to predict bat speed. For the first step, we have decided to run a Recursive Feature Elimination(RFE) on the 60 potential features from our dataset to pull out a smaller number of strong predictive features to use in our model. Next, using those select features, we will run a regression analysis to create a model that can be used to predict a player’s bat speed. Let’s take a closer look at these analyses.\n\n\nValues Statement\nThis project will mainly be utilized by coaches or anybody concerned with player development in baseball. The information that our model would provide to coaches would allow them to make better coaching decisions. In addition to helping coaching staffs, the players themselves would also benefit from more directed training and better evaluation standards. The baseball offseason is very short, so being able to make the most of this time is extremely valuable.\nThis dataset is the first of is the first of its kind to be released as open source. When open-source pitch physics data first became available, it fundamentally changed the way in which baseball was viewed and played. This information allowed both pitchers and hitters to have reliable and measurable feedback on every pitch. The availability of pitch-level biomechanics data has the potential to once again fundamentally change baseball. By working on this project, we are contributing to the larger effort of the baseball community to better understand exactly what makes a baseball swing productive.\n\n\nMaterials and Methods:\n\n\nData\nFrom the OBP dataset, we will be focusing on baseball-hitting data, specifically force plate and joint velocity to predict bat speed. The original datasets can be found here. Driveline baseball research collected this data using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. Originally, both the force plate and joint velocity datasets had over 1,000,000 observations, with each individual swing including thousands of observations because the swing was broken down by milliseconds. We felt it was unnecessary to keep the time aspect of the dataset, as the velocities produced for each feature variable were very similar from millisecond to millisecond, and the large datasets were difficult to work with. To get rid of the time component and obtain a more reasonably sized data set, we found the minimum, maximum and range of each feature variable in the dataset for every swing. Each swing is labeled by session_swing in our dataset, and each row is a different swing. The session swing is labeled by player ID and swing number, for example, session_swing 111_3 is player 111’s third swing. Not all players have the same number of swings in the dataset, but we don’t think this should have any impact on our results. After eliminating the time aspect, each swing has 60 potential feature variables. The 60 feature variables include the min, max and range of the forces produced by many different body parts in the x, y and z directions during a player’s swing. Some examples include lead shoulder which is the player’s shoulder closest to the pitcher, and rear hip which is the player’s hip furthest from the pitcher.\nOur data possesses some limitations as it exclusively represents male baseball players and doesn’t include any data from female softball players. We think it would be interesting for Driveline baseball research to expand to softball to eliminate some of the gender bias they have inadvertently caused.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination, or RFE is a recursive algorithm that is used to find the most impactful features out of a large feature set. This is accomplished by training machine learning on all the features, and then removing the least impactful features. This process is repeated with the smaller feature set, until the feature set is of the desired size. This can help prevent overfitting and allow for easier use and training. This does require that the model it is being used to select features has a way to calculate the effect of features, which means that it won’t work for every model, or some determinator has to be created for it to be used. Another drawback is that unless proper testing is done to find out the amount of impactful features, the accuracy can be diminished beyond the benefits of avoiding overfitting.\n\n\nMultiple Linear Regression\nBecause our project is concerned with predicting bat speed, we require a numeric prediction model, rather than a classification prediction model. We decided to use Multiple Linear Regression, which allows us to take two or more features in a dataset to predict a single numeric dependent variable, bat speed. Multiple Linear Regression differs from regular Linear Regression in that you can use more than one feature to predict the target variable. Once built, we can isolate each individual feature and evaluate its impact on the target variable.\nWith our linear regression model, we will be using the Mean Squared Error (MSE) loss function to determine the accuracy and performance of our model.\n\n\nRandom Forest Regression\nRandom Forest Regression is a technique that creates multiple decision trees and averages their outputs to give a final result that often has a high prediction/classification rate. The process involves the user selecting the amount, n, of decision trees to be created, then using the bootstrapping method to randomly select k data samples from the training set. (Bootstrapping is simply the process of randomly selecting subsets from a dataset over a certain number of iterations and a certain number of variables, then the results from each subset are averaged together which returns a more powerful result.) Then, create n decision trees using different random k values. Using the decision trees, predict regression results that can be used on unseen data. Finally, the regression results are all averaged together, returning a final regression output. Random Forests generally provide high accuracy for feature selection as it generates uncorrelated decision trees built by choosing a random set of features for each tree.\n\n\nVariable Overview\nThe features which we have created our data set with fall into two main categories: biomechanics data and force plate data. Beginning with the biomechanics data, we have a set of joints and their associated angular velocities in three planes of motion. We have information on lead hip, lead shoulder, rear hip, rear shoulder, pelvis, torso, and the torso-pelvis complex. For each of these joints, we calculated the range of velocities and maximum velocities for each swing captured. With the force plate data, the lead foot and rear foot are independently observed, and the data is split among the three planes of motion along the x, y, and z axes. For each pairing of foot and plane of motion, we calculated the minimum, maximum, and range of the force produced.\n\n\nThe Process\n\nStep 1: Cleaning the Data\nOur original dataset contained over 1,000,000 observations that were grouped by session_swing. Each swing contained hundreds of observations that analyzed a variety of features over time (~0.0028 seconds between captures). For our project, we wanted to remove this time aspect and instead create a simplified dataset that contained the minimum, maximum, and range values of the features of interest for each swing.\nTo do so, we imported our dataset in R and grouped it by the session_swing variable, and, by using base R functions, calculated the minimum, maximum, and range of each feature variable of interest. We repeated this for the force plate dataset and joint velocity dataset, then used left-join to combine the two datasets to create a conglomerate dataset with all potential predictive features for each session_swing variable.\nWe then added our target vector, max bat speed, from POI_metrics.csv to create our fill dataset that includes our target vector.\nThis process allowed us to get reduce the size of our dataset from over 1,000,000 observations to 665 session_swings.\n\n\nStep 2: RFE Feature Selection\nWe used SKLearn’s RFE feature collection class, which can be found here.\nThe RFE model from the SKLearn class allowed us to gain the base understanding we needed to implement our own version of RFE. After reading through the API and playing around with the RFE feature from SKLearn, we decided to implement our own version of RFE to use with the linear regression model we also implemented. Our RFE function tkaes in three parameters: a feature matrix, X, a target vetor, y, and the number of features we want to be selected, k. The function uses a nested for loop to run through all values i in 1:k and at each iteration, j, checks the weight of all the remaining features that were fit on the linear regression model. From here, the best features are selected as the features with the minimum absolutle value of weight. We use this function in conjunction with our linear regression model to find the number of feautres within our dataset that most accuratley predict the batspeed of a player.\n\n\nStep 3: Building the Models — Multiple Linear Regression and Random Forest\nWe decided to implement our own linear regression model similar to the work we did in class, selecting just the analytic version. We were able to pull out score and predict functions from our previous blog post implementation. We had to modify our fit function by adding a regualarization term to the weight vector to help avoid over/under-fitting the model.\n\n\nStep 4: Testing the Models: Linear Regression and Random Forest\nTo test and train the models, we used an 80/20 train/test split. For both models, we ran a loop to show our the training and testing scores while increasing the number of selected features for the recursive feature elimination model. Once we identified the optimal range of features to use on the Multiple Linear Regression and Random Forest Regression models, we created subsets of our training and testing data to contain the selected features. Finally, we trained and tested our models on the data subsets (80 train/20 split).\n\n\n\nResults and Conclusion\n\nLinear Regression\nBelow shows the effect of increasing the number of features on the accuracy scores produced by the Linear Regression model during RFE. As we can see, our training and testing accuracy scores tend to increase as the number of features increase.\nWe noticed that our model tends to have a higher testing accuracy when the model is ran on fewer features, and has the best training score when the model uses all 60 features. However, because we wanted to identify 10-20 key features, we decided to train and test our model on the 13 best features, which we selected as value that yields the second best testing score.\nThe table shows the 13 most important features as selected by our RFE function. These results were quite unexpected, including values like min_rfx and min_rfy as selected variables of importance. min_rfx and min_rfy represent the minimum rear force produced in the x and y directions, which essentially represent the load, or backwards movement prior to the actual swing itself. Other variables make sense as being some of the most important features, such as range_lead_hip_z, and max_torso_pelvis_x as these are body parts that are essitial in creating the rotational force of a swing to help produce a better bat speed.\nUsing a subset of these 13 features, we trained and tested our model, which produced a testing accuracy score of 38.7%. Unfortunately, this low accuracy indicates that our model isn’t performing as we had hoped. It could mean that our data doesn’t have a strong linear seperability which indicates there is nothing wrong with our model, but rather it isn’t the best model option for our data. Becasue of this, we decided to see if we could produce stronger results by using the Random Forest Regression model.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"biomechanics_dataset_v1.csv\") \n\nnp.random.seed(1)\nX = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n\n\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Scores Produced by Linear Regression Model During RFE\")\nplt.show()\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]])\n\n\nx_train_lin = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test_lin = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train_lin, y_train)\nprint(\"Testing Accuracy of Subset:\", lr.score(x_test_lin, y_test))\n\n\n\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\nTesting Accuracy of Subset: 0.386607442390802\n\n\n\n\nRandom Forest Regression\nBelow is a graph that displays the training and testing scores produced by the Random Rorest Regression model when ran with RFE over all of the features. As we can see from the graph, this model performed much better than the Linear Regression model. Unlike the Linear Regression model, the Random Rorest model reached a peak around 10 features and maintained consistency at that score, whereas the Linear Regression model had increased variance between scores across all of the features.\nBecause the Random Forest Regression model creates a series of trees using the bootstrapping method, we expected the model to have a better training accuracy and weren’t as concerned with overfitting. We also expected it to have a better accuracy score than the Linear Regression model because it can capture non-linear relationships, which we expect our data to have due to the poor performance of the Multiple Linear Regression model. Aditionally, because of the way the trees are built, the random forest model is less likely to be heavily affected by outliers in the data which will allow it to have a better testing accuracy score.\nWe found that the Random Rorest Regression model had the higher training and validation scores than the Linear Regresion model. The training score reached nearly 100% when ran with more than 10 features. The testing accuracy reached around 65% at 10 features and had a slight increase as the number of features increased. To compare this model with the Linear Regression model, we chose the 15 most important features to test and train the Random Forest model. After training and testing our model on the subset of 15 features, we got a testing accuracy score of 55.4%, which is significantly better than our multiple linear regression model. Considering there is no option for our model to randomly guess since we are predicting a continuous numeric value, we are satisfied with the amount our random forest model learned.\nInterestingly, there is only one feature that the Linear Regression model and the Random Forest Regression model both selected: max_rear_shoulder_y. This feature captures the top hand on the bat, so if the shoulder isn’t moving in the swing, it will hinder the ability to produce enough rotational force from the torso.\nWe were suprised to find that, of the 13 features selected by the Linear Regression model and of the 15 features selected by the Random Forest Regression model, there weren’t more similarities amongst the selected features between the two models. We were hoping to discover a few select features that both models identified as important features, and were interested to find that the majority of the selected features were different.\nWe hypothesize that the dimentionality of the forces produced by each body part may be a factor that contributes toward the challenge of distinguishing significant features. For example, in the Linear Regression model, max_toros_y was selected as an important feautre, and in the Random Forest Regression model, max_torso_z was selected as an important feature. This indicates that the max force produced by the torso is important when creating and predicting bat speed. If we were to run this experiment again, we may try getting rid of the x, y and z components and just use the average of the forces produced by that body part.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores_test = []\nscores_train = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores_test.append(estm.score(x_test,y_test))\n    scores_train.append(estm.score(x_train,y_train))\n    \nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores_test, label='Testing accuracy')  # Plot the chart\nplt.plot(x_val, scores_train, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Random Forest Regression\")\nplt.legend()\nplt.show() \n\n\nfrom RandomForestRegressor import RandomForest\nrf = RandomForest()\n\nx_train_rf = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test_rf = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train_rf, y_train, 1000,500)\nprint(rf.score(x_test_rf, y_test))\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]])\n\n0.5541368058167476\n\n\n\n\n\n\n  \n    \n      \n      range_lead_hip_x\n      range_pelvis_y\n      range_rear_shoulder_x\n      range_torso_z\n      range_torso_pelvis_z\n      max_lead_shoulder_z\n      max_pelvis_z\n      max_rear_shoulder_y\n      max_torso_z\n      max_rfz\n      max_lfx\n      range_rfy\n      range_rfz\n      range_lfy\n    \n  \n  \n    \n      0\n      590.6812\n      371.0611\n      838.0101\n      848.3957\n      743.5585\n      617.1386\n      733.6451\n      514.1198\n      775.7749\n      1101.3711\n      121.2052\n      240.6389\n      1052.5648\n      545.3695\n    \n    \n      1\n      536.1970\n      393.4254\n      947.9660\n      814.2556\n      642.8480\n      751.1699\n      799.8748\n      535.2822\n      775.3766\n      1092.3006\n      111.2187\n      297.5680\n      1040.9895\n      603.8510\n    \n    \n      2\n      586.8320\n      396.8130\n      801.1592\n      823.2495\n      853.6754\n      723.6880\n      740.7065\n      472.9189\n      793.0441\n      1117.9434\n      178.4852\n      351.2961\n      1002.5322\n      599.4007\n    \n    \n      3\n      628.4384\n      402.3244\n      958.8471\n      870.6640\n      541.5395\n      810.9479\n      741.3719\n      484.2663\n      819.9890\n      1102.4140\n      170.5486\n      344.0314\n      1094.4491\n      617.4273\n    \n    \n      4\n      595.3172\n      348.1626\n      840.4242\n      809.9368\n      756.6446\n      862.6313\n      770.4950\n      502.2202\n      774.5865\n      1119.0327\n      176.3782\n      262.0008\n      1100.7345\n      597.7741\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      825.4631\n      350.8260\n      1015.7101\n      919.6701\n      587.4149\n      838.6431\n      679.5463\n      488.3580\n      859.0192\n      947.5325\n      69.2794\n      380.5133\n      916.4106\n      464.0069\n    \n    \n      633\n      768.7166\n      377.7951\n      1020.7830\n      910.0289\n      716.7429\n      799.7780\n      715.3288\n      477.7701\n      846.6447\n      958.0700\n      60.6210\n      365.7219\n      931.9138\n      471.4245\n    \n    \n      634\n      667.8735\n      366.3885\n      978.3792\n      880.3159\n      516.4786\n      815.2906\n      701.0455\n      407.3251\n      820.1794\n      998.6667\n      56.2369\n      331.5945\n      954.3035\n      441.1844\n    \n    \n      635\n      698.0434\n      390.7154\n      1029.8385\n      896.8795\n      616.6955\n      604.2217\n      681.9455\n      463.8467\n      838.6638\n      939.1254\n      67.6610\n      383.2294\n      909.9346\n      533.1048\n    \n    \n      636\n      764.1730\n      348.0633\n      1024.9879\n      907.0956\n      606.3088\n      830.5895\n      677.4534\n      505.9937\n      846.7994\n      935.7064\n      91.0192\n      346.1590\n      916.5282\n      403.0329\n    \n  \n\n637 rows × 14 columns\n\n\n\n\n\nConcluding Discussion\nOverall, our project was sucessful, as we built two models and an RFE function to help us determine the most important features of a swing while predicting bat speed. When we formulated the idea for our project idea, our goal was to deliver python source code that we constructed, along with two jupyter notebooks. One that contained our intial exploration, and one that held our final write up and experiments. We were sucessful in meeting this goal, as we finished with more than two jupyter notebooks, and two python source code files that contained our linear regression with RFE and our random forest model.\nIf we had more time, we would work on finding ways to improve the accuracy of our models. One idea we have to improve accuracy score is to get rid of the dimension factor, as we mentioned above, so each biomechanic force only has one representation instead of three. We hope this would help our models narrow down the important features and produce a better accuracy score. Additionally, we would like to bring in more data to train and test our model on. DriveLine baseball technology is relatively new, so the data is sparse. If the technology was more accesible, we could have more data which would allow our model to improve its success.\n\n\n\nBias\nWe created a model that predicts a player’s bat speed with approximately 65% accuracy, using around 10 features. As with any algorithm, we must consider any assumptions or biased data that could have resulted in systematic error throughout our process. Because our training data came from the Driveline Research and Development, our model was only trained on the players with close proximity or eligibility to their biomechanics facility. According to the U.S. Census, 43.3% of the population of Kent, Washington (home of Driveline R&D) are white individuals, contrasted with 12.2% of the population being black individuals. While the data’s goal is to highlight specific bodily forces and movements that contribute to predicting bat speed rather than demographics like race, age, height, or weight, we must acknowledge that this data is most likely skewed toward white individuals and could be less accurate in predicting the bat speed of players of different races.\nAdditionally, we must highlight that baseball is a male-dominated sport, with the rare exception of a few women playing the sport — see article on Alexis Hopkins. While sports are typically gender segregated for the sake of “fairness” and an acknowledgment that male and female bodies are inherently different and will perform as such, factors like absolute strength and size are not as important in the sport of baseball, as they might be in gender-segregated sports like football and soccer. Rather, the Women’s Sports Foundation explains that baseball involves skills that are combinations of timing, coordination, strength, knowledge of the game, strategies, and control, and argues that bat speed and bat control are more important than absolute strength.\nYet, despite all of this, the Driveline R&D data only contains the biomechanics of male batters. Therefore, if our model were to be improved and implemented, it would only perpetuate the segregation of men and women in this sport. If the data to improve a player’s bat speed can only improve male players, women will continue to be left in the dust.\nDriveline Baseball describes its mission as developing the future of baseball training by finding industry-leading insights; all while publishing those insights back to the baseball community where they belong. However, because baseball is a historically white and male-dominated sport, the “insights” that will be found will only contribute to solidifying that the “baseball community” remains dominated by players that fit those demographics.\nIt is our duty to expand this research and development into more marginalized player communities, such as female athletes and athletes of other races. Then, we can use these insights to create unique training programs that empower and embrace their unique features and help them become the best athletes they can be.\n\n\nApplication\nOur bat speed model could be used by coaching staffs to better inform the decisions that they make. For example, if a given player’s predicted bat speed is higher than their actual recorded bat speed, this would indicate a mechanical inefficiency in their swing. Coaches, with this knowledge, could then direct their focus to finding these mechanical ineffeciencies and correcting them. Players in this group would spend more time on skill acquisition training. On the other hand, if a player has a predicted bat speed which is equal to or lower than their actual bat speed, this would indicate above average efficiency in their swing and their training could be directed more towards general strength and power to increase force production. In either case, our model would help streamline the process from data collection to impact, giving coaches and players the power to have impactful training sessions tailored to each individual.\n\n\nGroup Contribution Statement\nCece Ziegler: Helped with data cleaning. Built RFE function and Linear Regression model. Performed RFE and model experiments. Led writing “Results and Conclusion” sections.\nDavid Byrne: Introduced topic. Managed data cleaning. Led writing of “Abstract and Overview of Significance of Topic”, “Data”, “Variable Overview”, “Values Statement”, and “Application” sections.\nJulia Fairbank: Led writing of “Formal Introduction to Topic”, “Recursive Feature Elimination”, “Multiple Linear Regression”, “Random Forest Regression”, “The Process”, and “Bias” sections.\nSam Ehrsam: Conducted initial RFE experiments with SKLearn library. Built Random Forest Regression model. Performed RFE experiments."
  }
]