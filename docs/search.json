[
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "from perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nWe are looking at the perceptron algorithm in this post, understanding how it operates and converges, and observing some use cases. The source code for the implementatio can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/perceptron-blog-post/perceptron.py\n\n\nThe perceptron algorithm I wrote has two main methods. First, I wrote a “fit” function which prepares our raw data for use in computation moving forward. The fit function first (say that 5 times fast) initializes an empty history vector, which we use to store the accuracy score of our algorithm after each iteration. We also initialize our weight vector w, and append a column of ones to our feature matrix X, which makes our computation of dot product far easier. The second major method is the perceptron update function, which is responsible for the incremental adjustments to our weight vector. This function operates as follows:\n1. Choose a random point in X 2. Compute the dot product of the given point and the current weight vector w 3. If the dot product is less than 0 update w as follows:\n- \\(w^{t+1} = w^t + (2y_i-1) * X_i\\)\n\n\n\n\n\nOur first experiment is a simple 3 dimensional case that is linear seperable.\n\nnp.random.seed(11)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can run our algorithm until convergence, print the resulting score, and plot a line using the resulting weight vector. The process is as follows: 1. Initialize our Perceptron object and fit the model 2. For the specified number of iterations\na. Run the predict function, which creates a vector of guesses \\(\\hat{y}\\)\nb. Score the current iteration and store the score value in the history vector\nc. Check if the score is equal to 1 and terminate the loop, otherwise perform the perceptron update step\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 1.0\n\n\n\n#Line drawing function using weight vector w\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can also plot the progession of the score of our algorithm for each iteration. In this case we reach a score value of 1.0 before the max iterations\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn the case of data which is not linearly seperable, the perceptron algorithm does not converge. Convergence occurs in this case when we reach a perfect score, which is impossible here. Data such as this looks as follows:\n\nnp.random.seed(15)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe again can run our algorithm and plot our final weight vector as well as the score for each iteration.\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 0.97\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this case, since convergence is impossible, we reach the max number of iterations before converging to a score of 1.0.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn this case, we increase the number of features from 3 to 5\n\nnp.random.seed(21)\n\nn = 100\np_features = 5\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -1.7, -1.7), (1.7, 1.7, 1.7, 1.7)])\n\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 1.0\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nDespite increasing the number of features, the perceptron algorithm can still converge. The dot product is simply calculated using more features, and the same update rule can be applied.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Perceptron Algorithm\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is David’s fun little blog"
  }
]