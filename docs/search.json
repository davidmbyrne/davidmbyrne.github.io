[
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "from perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nWe are looking at the perceptron algorithm in this post, understanding how it operates and converges, and observing some use cases. The source code for the implementation can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/perceptron-blog-post/perceptron.py\n\n\nThe perceptron algorithm I wrote has two main methods. First, I wrote a “fit” function which prepares our raw data for use in computation moving forward. The fit function first (say that 5 times fast) initializes an empty history vector, which we use to store the accuracy score of our algorithm after each iteration. We also initialize our weight vector w, and append a column of ones to our feature matrix X, which makes our computation of dot product far easier. The second major method is the perceptron update function, which is responsible for the incremental adjustments to our weight vector. This function operates as follows:  1) Choose a random point in X  2) Compute the dot product of the given point and the current weight vector w  3) If the dot product is less than 0 update w as follows:  - \\(w^{t+1} = w^t + (2y_i-1) * X_i\\)\n\n\n\n\n\nOur first experiment is a simple 3 dimensional case that is linear seperable.\n\nnp.random.seed(11)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can run our algorithm until convergence, print the resulting score, and plot a line using the resulting weight vector. The process is as follows: 1) Initialize our Perceptron object and fit the model 2) For the specified number of iterations\na. Run the predict function, which creates a vector of guesses \\(\\hat{y}\\)\nb. Score the current iteration and store the score value in the history vector\nc. Check if the score is equal to 1 and terminate the loop, otherwise perform the perceptron update step\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 1.0\n\n\n\n#Line drawing function using weight vector w\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can also plot the progession of the score of our algorithm for each iteration. In this case we reach a score value of 1.0 before the max iterations\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn the case of data which is not linearly seperable, the perceptron algorithm does not converge. Convergence occurs in this case when we reach a perfect score, which is impossible here. Data such as this looks as follows:\n\nnp.random.seed(15)\n\nn = 100\np_features = 3\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n#Plot points\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe again can run our algorithm and plot our final weight vector as well as the score for each iteration.\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 0.97\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this case, since convergence is impossible, we reach the max number of iterations before converging to a score of 1.0.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nIn this case, we increase the number of features from 3 to 5\n\nnp.random.seed(21)\n\nn = 100\np_features = 5\n\n#Create feature matrix X and vector y\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -1.7, -1.7), (1.7, 1.7, 1.7, 1.7)])\n\n\np = Perceptron()   \np.fit(X, y) #Fit model\nmax_iter = 1000 #Set max iterations\nfor i in range(max_iter):\n    p.predict(p.X_) #Make predictions\n    score = p.score(X, y) #Score current weight \n    p.history.append(score)\n    if score == 1:\n        break\n    elif score < 1:\n        p.perceptron_update(p.X_, y, p.w_) #Update weight vector if score is not 1.0\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 1.0\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nDespite increasing the number of features, the perceptron algorithm can still converge. The dot product is simply calculated using more features, and the same update rule can be applied.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\nFor each iteration of our perceptron update we have to compute a dot product of two vectors, each with length p equal to the amount of features. In computing this dot product, we have to make p multiplications and p-1 additions. If we assume each of these operations to be time constant operations, the time compelxity of one step is 2p-1 or O(p)."
  },
  {
    "objectID": "posts/gradient-descent-blog-post/LRtest.html",
    "href": "posts/gradient-descent-blog-post/LRtest.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "class LogisticRegression:\n    \n    def __init__(self):\n        pass\n    \n    def score(self, X, y):\n        return np.sum((y == np.transpose(self.guess)))/(X.shape[0])\n    \n    def predict(self, X, w):\n        self.guess = (np.dot(X, w) >= 0).astype(int)\n        return self.guess\n        \n    def fit(self, X, y):\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    def sigmoid(self, z):\n        return 1 / (1+ np.exp(-z))\n    \n    def logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n    \n    def empirical_risk(self, X, y, loss, w):\n        y_hat = self.predict(X, w)\n        return loss(y_hat, y).mean()\n    \n    def gradient(self, w, X, y):\n        w = w.reshape(3,1)\n        sigdot = (np.dot(X, w) >= 0).astype(int)\n        return np.sum(np.multiply(X,(self.sigmoid(sigdot) - y.reshape(len(y),1))))\n    \n    def fit_stochastic(self, X, y):\n        prev_loss = np.inf\n        self.loss_history = []\n        self.score_history = []\n        w = np.random.rand(2,1)\n        bias = np.random.uniform(0,1)\n        self.w_ = np.append(w, -bias)\n        self.X_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        n = X.shape[0]\n        for j in np.arange(m_epochs):\n            \n            order = np.arange(n)\n            np.random.shuffle(order)\n\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = self.X_[batch,:]\n                y_batch = y[batch]\n                grad = self.gradient(self.w_, x_batch, y_batch) \n                self.w_ -= alpha*grad                      \n            new_loss = self.empirical_risk(self.X_, y, self.logistic_loss, self.w_)\n            self.loss_history.append(new_loss)\n            self.currentscore = self.score(LR.X_, y)\n            self.score_history.append(self.score)\n    \n            if np.isclose(new_loss, prev_loss):          \n                break\n            else:\n                prev_loss = new_loss\n        \n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()  \nalpha = 0.001\nprev_loss = np.inf\nLR.fit(X, y)\nfor i in range(1000):\n    LR.w_ -= alpha*LR.gradient(LR.w_, LR.X_, y)                      \n    new_loss = LR.empirical_risk(LR.X_, y, LR.logistic_loss, LR.w_)\n    LR.loss_history.append(new_loss)\n    score = LR.score(LR.X_, y)\n    LR.score_history.append(score)\n    \n    if np.isclose(new_loss, prev_loss):          \n        break\n    else:\n        prev_loss = new_loss\n    \nprint(\"final score: \" + str(score))\n\nfinal score: 0.92\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nn = X.shape[0]\nfor j in np.arange(m_epochs):\n            \n    order = np.arange(n)\n    np.random.shuffle(order)\n\n    for batch in np.array_split(order, n // batch_size + 1):\n        x_batch = X[batch,:]\n        y_batch = y[batch]\n        grad = gradient(w, x_batch, y_batch) \n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.001\nbatch_size = 5`\nLR.fit_stochastic(X, y)\nprint(\"Final Score: \" + str(LR.currentscore))\n\nFinal Score: 0.93\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is David’s fun little blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gradient-descent-blog-post/index.html",
    "href": "posts/gradient-descent-blog-post/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "from logisticregression import LogisticRegression\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons\nimport random\n\n\n\n\nSource code for this post can be found at https://github.com/davidmbyrne/davidmbyrne.github.io/blob/main/posts/gradient-descent-blog-post/logisticregression.py\n\n\n\nOne of the variables that gradient decent utlizes is the learning rate, or alpha. We utilize this value in calculating a new weight vector \\(w\\), and it determines the impact that the gradient has in each step of modifying \\(w\\). A small value of alpha will make the steps from each iteration to the next smaller, while a larger alpha will cause larger jumps. When choosing an effective value for alpha, it is important to consider this balance. A very small alpha will cause the minimization of the loss function to take a long time to converge, but too large of an alpha value will make it impossible to converge at all. We can observe this case of non convergence.\n\nrandom.seed(14)\nX, y = make_moons(n_samples = 100, noise = 0.35)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 0.1\n\nLR.fit(X, y, m_epochs, alpha)\nprint(\"Converged after \" + str(LR.stop) + \" tries\")\n\nConverged after 3 tries\n\n\n\n\n\n\nrandom.seed(14)\nLR = LogisticRegression()\nm_epochs = 1000\nalpha = 5\n\nLR.fit(X, y, m_epochs, alpha)\nprint(\"Converged after \" + str(LR.stop) + \" tries\")\n\nConverged after too many tries\n\n\n\n\n\n\nIn addition to our typical run of the mill gradient descent, we can implement stochastic gradient descent, which divides the full data set into “batches” or subsets of a given size and calculates the gradient over just that batch. We can use this to our advantage, as manipulating batch size can affect the speed at which the loss function converges.\n\np_features = 20\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 5\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 2.89\n\n\n\nLR = LogisticRegression()\n\nterm_arr = []\nm_epochs = 1000\nalpha = 0.05\nbatch_size = 20\nfor i in range(100):\n    LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n    term_arr.append(LR.stop)\nprint(\"The average amount of epochs to convergence was \" + str(sum(term_arr)/100))\n\nThe average amount of epochs to convergence was 5.04\n\n\nIn this case, using a smaller batch size of 5 led to convergence in nearly half of the total epochs when compared to a batch size of 20. To see a greater trend, we can try all batch sizes from 2 to 30.\n\ntracker = []\nfor i in range(2,30):\n    term_arr = []\n    m_epochs = 1000\n    alpha = 0.05\n    batch_size = i+2\n    for i in range(100):\n        LR.fit_stochastic(X, y, m_epochs, alpha, batch_size)\n        term_arr.append(LR.stop)\n    tracker.append(sum(term_arr)/100)\nfig = plt.plot(tracker)\nxlab = plt.xlabel(\"Batch Size\")\nylab = plt.ylabel(\"Average Epochs\")\n\n\n\n\nThe larger experiment confirms the previous belief, and smaller batch sizes have faster convergence times, with the change plateauing after a batch size of about 15."
  }
]