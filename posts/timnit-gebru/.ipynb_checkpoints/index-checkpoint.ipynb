{
 "cells": [
  {
   "cell_type": "raw",
   "id": "75f41a4c-e28e-477c-add9-2f40e25a2104",
   "metadata": {},
   "source": [
    "---  \n",
    "title: Dr. Timnit Gebru\n",
    "author: David Byrne\n",
    "date: '2023-03-09'\n",
    "image: \"image.jpg\"\n",
    "description: \"Discussing our meeting with Dr. Timnit Gebru\"\n",
    "format: html  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9aa429-d1ff-4418-bfb4-f076bf312e51",
   "metadata": {},
   "source": [
    "## Engaging with Dr. Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0a273-09d7-4061-b19b-1c1b4e365269",
   "metadata": {},
   "source": [
    "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics. \n",
    "\n",
    "#### FATE in Computer Vision\n",
    "The video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s.\n",
    "In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy. \n",
    "\n",
    "Dr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\n",
    "\n",
    "Dr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\n",
    "\n",
    "Dr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well. \n",
    "\n",
    "tl;dr With great power comes great responsibility\n",
    "\n",
    "#### A question\n",
    "Do you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdfe4e-6419-4031-a3f9-5a7fd5e3e446",
   "metadata": {},
   "source": [
    "## Dr. Gebru's talk at Middlebury\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08713e6d-b53c-4e8a-a2bb-f60b1faa82eb",
   "metadata": {},
   "source": [
    "I was unable to attend Dr. Gebru's talk to due to a prior commitment, but I was able to rewatch the recording of her talk after the fact. Dr. Gebru spoke in depth about the implications of AI and LLM development, focusing in particular on understanding both the indivuduals responsible for the advancements as well as their motivation for their actions. She began her talk by contrasting the different perspectives between the silicon valley billionaires creating these models and the much larger group of individuals responsible for helping with data processing. For example, OpenAI used manual data labeling and flagging to monitor and moderate the outputs of Chat-GPT. In addition to moderating harmful outputs, these outsourced laborers in Kenya were also paid 2 dollars an hour. In contrast, the heads of these companies, like OpenAI, are recieving billions of dollars in funding.  \n",
    "\n",
    "She then transitioned into discussing what the end goal of creating these AGI models truly is. Dr. Gebru highlighted the ties between second wave eugenics and the organizations and people responsbile for driving effort and funding towards AGI development. She focused in particular on a conglomerate of identies which she called the \"TESCREAL Bundle.\" This collection of groups or identities is well represented in the field of AI research. These individuals, from Transhumanists to Cosmists and beyond, are all somewhat working towards what Emile Torres calls, \"techno-utopianism + a sense that one is genuinely saving the world.\" The main issue that Dr. Gebru calls into question is the idea of \"utopia for whom?\" These individuals who claim to want to create this so-called utopia, whether intentional or not, are perpetuating biases and discriminatory structures. The utopia that they speak of is currently being built on the backs of marginialized and impoverished communities. In addition, the systems that are creating have so far proven to be discriminatory and harmful in action. Utopia for these individuals has taken the form of authoritarianism and the centralization of power. Resources and capital are being directed towards massive corporations rather than to communities in need. People's careers are being threatened by the narrative that these companies are peddling about AGI. The utopia that they are working towards is not an equitable one, rather one for the powerful and wealthy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c544ed8-da21-4889-93d0-300c0c2516a7",
   "metadata": {},
   "source": [
    "## My Thoughts\n",
    "\n",
    "I thought that Dr. Gebru highlighted some interesting connections between groups of individuals that helped me understand some of the motivation behind the current hype around AI. At times it felt like the conclusions she came to were built on some flimsy logic, but I don't think that it takes away from the value of what she was saying. In particular, highlighting the abuse of workers through the use of third parties for hiring was incredibly important and revealing information. It is easy to forget that these models do not appear out of thin air and there are actually people who are responsible for tasks such as content moderation which is not easy or pretty work. In regard to her argument about AGI and eugenics, I will have to read her paper that is currently being reviewed before I know where I stand. I believe that she has a valid point, but her presentation to us was scattered and difficult to follow. It felt rushed an incomplete, which is unfortunate. I would also like to do some more of my own research into the groups in the TESCREAL bundle to better understand who the individuals being quoted were. It seemed like she used a lot of problematic and discriminatory quotes that felt like extremist language, and I was left wondering if these individuals were indicitive of the perspective of the larger group or if they were more so an isolated corner. Regardless, Dr. Gebru's talk was certainly thought provoking and left me thinking deeply about the implications of the individuals that we assign as authorities or as powerful. Capital is inherently a powerful tool, and by pooling into the hands of only a few people, we allow them to shape narratives, often in their own favor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
