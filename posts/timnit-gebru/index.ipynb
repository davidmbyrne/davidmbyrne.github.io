{
 "cells": [
  {
   "cell_type": "raw",
   "id": "75f41a4c-e28e-477c-add9-2f40e25a2104",
   "metadata": {},
   "source": [
    "---  \n",
    "title: Dr. Timnit Gebru\n",
    "author: David Byrne\n",
    "date: '2023-03-09'\n",
    "image: \"image.jpg\"\n",
    "description: \"Discussing our meeting with Dr. Timnit Gebru\"\n",
    "format: html  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9aa429-d1ff-4418-bfb4-f076bf312e51",
   "metadata": {},
   "source": [
    "## Engaging with Dr. Timnit Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0a273-09d7-4061-b19b-1c1b4e365269",
   "metadata": {},
   "source": [
    "Dr. Timnit Gebru is an incredibly well respected expert in the field of AI and AI ethics, with years of expereince as a developer and researcher. As one of the premier voices in the space of AI ethics, we are incredibly lucky to have the experience of engaging with her at Middlebury. She is known for her contributions towards research on bias and fairness in machine learning, as well as advocating for a more diverse and inclusive tech industry. In her pursuit of research in AI ethics, Gebru co-authored a paper in 2020 which discussed the potential risks of large language models. The publishing of this paper would eventually lead to her wrongful termination from her then employer, Google. Following this, Dr. Gebru has continued her work as an independent researcher in the field of AI ethics. \n",
    "\n",
    "#### FATE in Computer Vision\n",
    "The video in question can be found at: https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s.\n",
    "In this talk, Dr. Timnit Gebru talks about current use cases of computer vision, as well as the problems and risks that come with their usage. One use case that she initally highlights is computer vision in hiring processes. Many companies have begun using computer vision algorithms to effectively score and evaluate potential candidates in virtual interviews. By offloading complex and nuanced human emotion to computer vision, we potentially can misread an individuals performance. It is one thing to identify that a person is smiling, it is another to label them as happy. \n",
    "\n",
    "Dr. Gebru then transitions into speaking about using computer vision in surveillance and policing. She highlights, in particular, the state of Maryland which has seen controversial usage of facial recognition in multiple cities police departments. In a more general sense, Dr. Gebru highlights the need to consider that these are people that are being used as data. By removing the humanity of the situation and abstracting it to just pure math, we lose purpose and perspective.\n",
    "\n",
    "Dr. Gebru also discusses the inherent bias in data collection which informs model building. In particular, she discusses the work of Gender Shades, which assesed the accuracy of gender classification tools based on skin type. This work found intersectional identities to have far higher rates of misclassification, due to a sampling issue. Models that were disproportionately trained on lighter skinned data struggled to correctly identify the gender of of darker skinned individuals. She advocates for diversity and inclusion in all aspects of AI, not just in results, but also in the methodology and data collection.\n",
    "\n",
    "Dr. Gebru wraps up her talk with a discussion of the social implications of building these models. For example, she makes the important distinction between a fair model and a just model. In the Maryland police situation, even if the model correctly identifies individuals regardless of race or gender, surveillance at this level is not just. Beyond invasion of privacy, a model such as this only upholds the inherently racist criminal justice system. We have a responsibility to make sure that models, even fair ones, do not disproportionately benefit privleged groups or harm marginalized ones. In other words, it is not ethical to simply evaluate fairness from a narrow point of view, rather, the societal context in which the model is being executed must be considered as well. \n",
    "\n",
    "tl;dr With great power comes great responsibility\n",
    "\n",
    "#### A question\n",
    "Do you forsee a way in which computer vision based models can be used to fight against discriminatory structures? It feels like even when work is done to mitigate bias, such as diversifying datasets, the structures that inform the data are so deep-seated that it is impossible to seperate it. The policing example in particular is interesting, because of course computer vision could be an effective tool for identifying criminals, but because the criminal justice system is so inherently discriminatory, using it feels unjust. Restating the question for clarity, can we find use cases for computer vision that actively fight against discriminatory structures rather than validate or support them?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
